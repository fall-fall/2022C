{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1579c577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 数据拆分任务启动 ---\n",
      "输入文件: ..\\..\\Data\\0\\处理后的数据.xlsx\n",
      "输出目录: ..\\..\\Data\\1\n",
      "\n",
      "成功加载总表，共 67 条数据，准备开始拆分...\n",
      "\n",
      "✅ 数据拆分逻辑执行完毕。\n",
      "  - 表1 (纯净未风化样本): 共 25 条\n",
      "  - 表2 (风化文物的风化点): 共 32 条\n",
      "  - 表3 (风化文物的未风化点): 共 10 条\n",
      "  - 校验: 25 + 32 + 10 = 67 (总数一致)\n",
      "\n",
      "✅ 所有新的数据表格已成功保存至指定目录。\n",
      "   -> ..\\..\\Data\\1\\表1_纯净未风化样本.xlsx\n",
      "   -> ..\\..\\Data\\1\\表2_风化文物的风化点.xlsx\n",
      "   -> ..\\..\\Data\\1\\表3_风化文物的未风化点.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ==============================================================================\n",
    "# 第一步：定义文件路径\n",
    "# ==============================================================================\n",
    "# 使用您指定的输入文件路径\n",
    "input_dir = Path('../../Data/0')\n",
    "input_file = input_dir / '处理后的数据.xlsx'\n",
    "\n",
    "# 使用您指定的输出目录路径\n",
    "output_dir = Path('../../Data/1')\n",
    "# 确保输出目录存在，如果不存在则自动创建\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"--- 数据拆分任务启动 ---\")\n",
    "print(f\"输入文件: {input_file}\")\n",
    "print(f\"输出目录: {output_dir}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 第二步：加载预处理好的总表\n",
    "# ==============================================================================\n",
    "try:\n",
    "    # 我们只关心已分类的数据表，因为未分类的没有风化信息\n",
    "    df_cleaned = pd.read_excel(input_file, sheet_name='已分类清洗后数据')\n",
    "    print(f\"\\n成功加载总表，共 {len(df_cleaned)} 条数据，准备开始拆分...\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ 错误：输入文件未找到，请确认路径 {input_file} 是否正确。\")\n",
    "    # 如果文件找不到，则停止执行\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"❌ 错误：读取文件时发生错误: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ==============================================================================\n",
    "# 第三步：执行拆分逻辑\n",
    "# ==============================================================================\n",
    "# --- (3.1) 拆分出 表1：纯净未风化样本 ---\n",
    "# 逻辑：直接筛选出 `表面风化` 为 \"无风化\" 的所有样本\n",
    "df_table1 = df_cleaned[df_cleaned['表面风化'] == '无风化'].copy()\n",
    "df_table1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# --- (3.2) 拆分出 表2 和 表3 ---\n",
    "# 逻辑：首先筛选出所有来自“风化”文物的样本，然后再根据采样点名称进行细分\n",
    "df_weathered_all_points = df_cleaned[df_cleaned['表面风化'] == '风化'].copy()\n",
    "\n",
    "# 添加一个辅助列'点类型'，用于区分风化点和未风化点\n",
    "# 这个逻辑基于我们之前根据题目规则的严格论证\n",
    "df_weathered_all_points['点类型'] = df_weathered_all_points['文物采样点'].apply(lambda x: '未风化' if '未风化' in str(x) else '风化')\n",
    "\n",
    "# 拆分出 表2：风化文物上的风化采样点\n",
    "df_table2 = df_weathered_all_points[df_weathered_all_points['点类型'] == '风化'].copy()\n",
    "df_table2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# 拆分出 表3：风化文物上的未风化采样点\n",
    "df_table3 = df_weathered_all_points[df_weathered_all_points['点类型'] == '未风化'].copy()\n",
    "df_table3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "print(\"\\n✅ 数据拆分逻辑执行完毕。\")\n",
    "print(f\"  - 表1 (纯净未风化样本): 共 {len(df_table1)} 条\")\n",
    "print(f\"  - 表2 (风化文物的风化点): 共 {len(df_table2)} 条\")\n",
    "print(f\"  - 表3 (风化文物的未风化点): 共 {len(df_table3)} 条\")\n",
    "print(f\"  - 校验: {len(df_table1)} + {len(df_table2)} + {len(df_table3)} = {len(df_cleaned)} (总数一致)\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 第四步：保存新的数据表格\n",
    "# ==============================================================================\n",
    "try:\n",
    "    # 定义输出文件名\n",
    "    file1_path = output_dir / '表1_纯净未风化样本.xlsx'\n",
    "    file2_path = output_dir / '表2_风化文物的风化点.xlsx'\n",
    "    file3_path = output_dir / '表3_风化文物的未风化点.xlsx'\n",
    "\n",
    "    # 将三个新的DataFrame分别保存为Excel文件\n",
    "    df_table1.to_excel(file1_path, index=False)\n",
    "    df_table2.to_excel(file2_path, index=False)\n",
    "    df_table3.to_excel(file3_path, index=False)\n",
    "\n",
    "    print(\"\\n✅ 所有新的数据表格已成功保存至指定目录。\")\n",
    "    print(f\"   -> {file1_path}\")\n",
    "    print(f\"   -> {file2_path}\")\n",
    "    print(f\"   -> {file3_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ 错误：保存文件时发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bbef11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 成功加载用于问题一预测模型的专属数据表 (表2 和 表3) ---\n",
      "✅ 成功构建回归模型的训练数据，共 2 对样本。\n",
      "\n",
      "--- 开始为每种化学成分训练并评估随机森林回归模型 ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     69\u001b[39m model = RandomForestRegressor(n_estimators=\u001b[32m100\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# 使用交叉验证评估模型性能\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m r2_scores = \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_regression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_current\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m mae_scores = cross_val_score(model, X_regression, y_current, cv=\u001b[32m5\u001b[39m, scoring=\u001b[33m'\u001b[39m\u001b[33mneg_mean_absolute_error\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# 打印简化的评估结果\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# print(f\"  - [{chemical}] R²均值: {np.mean(r2_scores):.4f}, MAE均值: {-np.mean(mae_scores):.4f}\")\u001b[39;00m\n\u001b[32m     77\u001b[39m \n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# 使用所有成对数据训练最终模型，并保存\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Conda\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Conda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:677\u001b[39m, in \u001b[36mcross_val_score\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[32m    675\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m cv_results = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[33m\"\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Conda\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Conda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:399\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[32m    398\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m results = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    419\u001b[39m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Conda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Conda\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Conda\\Lib\\site-packages\\joblib\\parallel.py:1911\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1908\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1910\u001b[39m \u001b[38;5;66;03m# Sequentially call the tasks and yield the results.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_dispatched_batches\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m   1913\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_dispatched_tasks\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Conda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:80\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = \u001b[43m(\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_with_config_and_warning_filters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarning_filters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdelayed_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(iterable_with_config_and_warning_filters)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Conda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:416\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[32m    398\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m    399\u001b[39m results = parallel\u001b[43m(\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    419\u001b[39m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Conda\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:404\u001b[39m, in \u001b[36m_BaseKFold.split\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m    402\u001b[39m n_samples = _num_samples(X)\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_splits > n_samples:\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    405\u001b[39m         (\n\u001b[32m    406\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCannot have number of splits n_splits=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m greater\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    407\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m than the number of samples: n_samples=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    408\u001b[39m         ).format(\u001b[38;5;28mself\u001b[39m.n_splits, n_samples)\n\u001b[32m    409\u001b[39m     )\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().split(X, y, groups):\n\u001b[32m    412\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "\u001b[31mValueError\u001b[39m: Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=2."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# ==============================================================================\n",
    "# 第一步：加载问题一专属数据\n",
    "# ==============================================================================\n",
    "# 定义文件路径\n",
    "input_dir = Path('../../Data/1')\n",
    "# 表2是模型的输入(X)\n",
    "table2_path = input_dir / '表2_风化文物的风化点.xlsx'\n",
    "# 表3是模型的标准答案(y)\n",
    "table3_path = input_dir / '表3_风化文物的未风化点.xlsx'\n",
    "\n",
    "try:\n",
    "    df_weathered_points = pd.read_excel(table2_path)\n",
    "    df_unweathered_points = pd.read_excel(table3_path)\n",
    "    print(\"--- 成功加载用于问题一预测模型的专属数据表 (表2 和 表3) ---\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ 错误：文件未找到。请确认 {e.filename} 是否存在于 {input_dir} 目录中。\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"❌ 错误：读取文件时发生错误: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ==============================================================================\n",
    "# 第二步：构建成对的回归训练集 (X 和 y)\n",
    "# ==============================================================================\n",
    "# 筛选出化学成分列\n",
    "chemical_columns = [col for col in df_weathered_points.columns if '(' in col and ')' in col]\n",
    "\n",
    "# 为了确保样本一一对应，我们使用'文物编号'作为索引进行合并\n",
    "df_weathered_indexed = df_weathered_points.set_index('文物编号')\n",
    "df_unweathered_indexed = df_unweathered_points.set_index('文物编号')\n",
    "\n",
    "# 将两个表横向合并，形成一个包含“风化后”和“风化前”成分的宽表\n",
    "paired_df = pd.merge(\n",
    "    df_weathered_indexed[chemical_columns],\n",
    "    df_unweathered_indexed[chemical_columns],\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    suffixes=('_风化后', '_风化前')\n",
    ")\n",
    "\n",
    "# 准备特征(X)：所有风化后的化学成分\n",
    "features_X = [col for col in paired_df.columns if '_风化后' in col]\n",
    "X_regression = paired_df[features_X]\n",
    "\n",
    "# 准备目标(y)：这是一个包含了所有风化前成分的DataFrame\n",
    "targets_y_all = [col for col in paired_df.columns if '_风化前' in col]\n",
    "y_regression_all = paired_df[targets_y_all]\n",
    "\n",
    "print(f\"✅ 成功构建回归模型的训练数据，共 {len(paired_df)} 对样本。\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 第三步：为每种化学成分训练并评估一个预测模型\n",
    "# ==============================================================================\n",
    "# 创建一个字典，用于存储我们为每种成分训练好的最终模型\n",
    "trained_regression_models = {}\n",
    "print(\"\\n--- 开始为每种化学成分训练并评估随机森林回归模型 ---\")\n",
    "\n",
    "# 循环为每种化学成分训练一个模型\n",
    "for chemical in chemical_columns:\n",
    "    current_target_col = chemical + '_风化前'\n",
    "    y_current = y_regression_all[current_target_col]\n",
    "    \n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # 使用交叉验证评估模型性能\n",
    "    r2_scores = cross_val_score(model, X_regression, y_current, cv=5, scoring='r2')\n",
    "    mae_scores = cross_val_score(model, X_regression, y_current, cv=5, scoring='neg_mean_absolute_error')\n",
    "    \n",
    "    # 打印简化的评估结果\n",
    "    # print(f\"  - [{chemical}] R²均值: {np.mean(r2_scores):.4f}, MAE均值: {-np.mean(mae_scores):.4f}\")\n",
    "    \n",
    "    # 使用所有成对数据训练最终模型，并保存\n",
    "    model.fit(X_regression, y_current)\n",
    "    trained_regression_models[chemical] = model\n",
    "\n",
    "print(\"✅ 所有化学成分的预测模型均已训练并评估完毕。\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 第四步：应用模型，生成包含预测结果的表格\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 正在应用已训练好的模型进行预测 ---\")\n",
    "\n",
    "# 我们要对`表2`中的所有风化点进行预测\n",
    "# 首先准备好这些样本的特征数据（确保列名与训练时一致）\n",
    "X_to_predict = df_weathered_points[chemical_columns]\n",
    "X_to_predict.columns = [col + '_风化后' for col in X_to_predict.columns]\n",
    "\n",
    "# 创建一个新的DataFrame来存储预测结果，先放入原始风化后的数据\n",
    "df_predictions = df_weathered_points[['文物编号', '文物采样点']].copy()\n",
    "df_predictions = pd.concat([df_predictions, X_to_predict.reset_index(drop=True)], axis=1)\n",
    "\n",
    "\n",
    "# 循环使用我们训练好的模型，对每个成分进行预测\n",
    "for chemical, model in trained_regression_models.items():\n",
    "    # 使用对应成分的模型进行预测\n",
    "    predicted_values = model.predict(X_to_predict)\n",
    "    # 将预测结果作为新列添加到结果表格中\n",
    "    df_predictions[chemical + '_预测风化前'] = predicted_values\n",
    "\n",
    "print(\"✅ 预测完成！\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 第五步：显示并保存最终的处理后表格\n",
    "# ==============================================================================\n",
    "# 为了方便在Notebook中直接查看，我们调整一下列的顺序，把对应的成分放在一起\n",
    "display_order = ['文物编号', '文物采样点']\n",
    "for chemical in chemical_columns:\n",
    "    display_order.append(chemical + '_风化后')\n",
    "    display_order.append(chemical + '_预测风化前')\n",
    "\n",
    "df_predictions_display = df_predictions[display_order]\n",
    "\n",
    "print(\"\\n--- 问题一模型处理后的最终表格 (部分预览) ---\")\n",
    "# 使用 .style.format 来美化输出，保留两位小数\n",
    "# .set_table_styles 使表格更易读\n",
    "styled_df = df_predictions_display.head().style.format(precision=2).set_table_styles(\n",
    "    [{'selector': 'th', 'props': [('background-color', '#0c5277'), ('color', 'white'), ('text-align', 'center')]},\n",
    "     {'selector': 'td', 'props': [('text-align', 'center')]}]\n",
    ")\n",
    "# 在Notebook中显示美化后的表格\n",
    "from IPython.display import display\n",
    "display(styled_df)\n",
    "\n",
    "\n",
    "# 保存完整的预测结果表格到新文件\n",
    "output_prediction_file = input_dir / '问题一预测结果对比表.xlsx'\n",
    "df_predictions_display.to_excel(output_prediction_file, index=False)\n",
    "\n",
    "print(f\"\\n✅ 包含预测结果的完整表格已成功保存至 -> 【{output_prediction_file}】\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
